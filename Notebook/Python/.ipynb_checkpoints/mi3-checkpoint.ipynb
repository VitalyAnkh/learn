{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "import sys\n",
    "import codecs\n",
    "from string import punctuation\n",
    "import jieba.posseg as pseg\n",
    "import jieba.analyse\n",
    "import jieba\n",
    "import collections\n",
    "from nltk import bigrams\n",
    "import math\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "#a = \"\"\"When the defendant and his lawyer walked into the court, some of the victim supporters turned their backs on him.  if we have more data then it will be more interesting because we have more chance to repeat bigrams. After some of the victim supporters turned their backs then a subset of the victim supporters turned around and left the court.\"\"\"\n",
    "\n",
    "\n",
    "# 预处理中文文本\n",
    "if sys.getdefaultencoding() != 'utf-8':\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding('utf-8')\n",
    "# 定义要删除的标点等字符\n",
    "add_punc = '，。、【 】 “”：；（）《》‘’{}？！⑦()、%^>℃：.”“^-——=&#@￥'\n",
    "all_punc = punctuation+add_punc\n",
    "\n",
    "# def cut_words(sentence):\n",
    "# print sentence\n",
    "#    return \" \".join(jieba.cut(sentence)).encode('utf-8')\n",
    "# 指定要分词的文本\n",
    "f = codecs.open('红楼梦-半半半本.txt', 'r', encoding=\"utf8\")\n",
    "# 指定分词结果的保存文本\n",
    "target = codecs.open(\"处理过的红楼梦-半半半本.txt\", 'w', encoding=\"utf8\")\n",
    "print('open files')\n",
    "line_num = 1\n",
    "line = f.readline()\n",
    "while line:\n",
    "\n",
    "    print('---- processing ', line_num, ' article----------------')\n",
    "    # 第一次分词，用于移除标点等符号\n",
    "# line=re.sub(r'[A-Za-z0-9]|/d+','',line)   #用于移除英文和数字\n",
    "    line_seg = \" \".join(jieba.cut(line))\n",
    "    # 移除标点等需要删除的符号\n",
    "    testline = line_seg.split(' ')\n",
    "    te2 = []\n",
    "    for i in testline:\n",
    "        te2.append(i)\n",
    "        if i in all_punc:\n",
    "            te2.remove(i)\n",
    "    # 返回的te2是个list，转换为string后少了空格，因此需要再次分词\n",
    "        # 第二次在仅汉字的基础上再次进行分词\n",
    "    line_seg2 = \" \".join(jieba.cut(''.join(te2)))\n",
    "    target.writelines(line_seg2)\n",
    "    line_num = line_num + 1\n",
    "    line = f.readline()\n",
    "f.close()\n",
    "target.close()\n",
    "\n",
    "f = open(\"处理过的红楼梦-半半半本.txt\")\n",
    "a = f.read()\n",
    "\n",
    "a1 = a.split()\n",
    "a2 = collections.Counter(a1)\n",
    "\n",
    "a3 = collections.Counter(bigrams(a1))\n",
    "a4 = sum([a2[x]for x in a2])\n",
    "a5 = sum([a3[x]for x in a3])\n",
    "a6 = {x: float(a2[x])/a4 for x in a2}  # word probabilities(w1 and w2)\n",
    "a7 = {x: float(a3[x])/a5 for x in a3}  # joint probabilites (w1&w2)\n",
    "u = {}\n",
    "v = {}\n",
    "for x in a6:\n",
    "    k = {x: round(math.log((a7[b]/(a6[x] * a6[y])), 2), 4)\n",
    "         for b in a7 for y in a6 if x and y in b}\n",
    "    u[x] = k[x]\n",
    "\n",
    "\n",
    "def calc_MI(x, y, bins):\n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "\n",
    "# bins = 5\n",
    "# X = {}\n",
    "# Y = {}\n",
    "# MI = {}\n",
    "# for x in X:\n",
    "#     for y in Y:\n",
    "#         MI[(x, y)] = calc_MI(x, y, bins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
